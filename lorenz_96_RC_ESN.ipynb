{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095f22bd",
   "metadata": {},
   "source": [
    "\n",
    "## Physics-Informed Neural Network for the Lorenz 96 System\n",
    "\n",
    "The Lorenz 96 system is a set of ordinary differential equations that is commonly used as a toy model in atmospheric science. It exhibits chaotic behavior, which makes it an interesting subject for study.\n",
    "\n",
    "In this notebook, we aim to build a Physics-Informed Neural Network (PINN) for the Lorenz 96 system. The idea behind a PINN is to combine the strengths of physics-based modeling with data-driven machine learning. By informing the neural network about the underlying physical laws (in this case, the Lorenz 96 equations), we hope to improve the model's accuracy and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3924abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c999781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Echo State Network (ESN) for Reservoir Computing (RC)\n",
    "# This class implements the reservoir computing approach using an Echo State Network. \n",
    "# Reservoir computing is a framework applied to recurrent neural networks where \n",
    "# the recurrent layer (the reservoir) is kept fixed, and only the readout weights are trained.\n",
    "\n",
    "class RC_ESN:\n",
    "    def __init__(self, input_dim, reservoir_size, output_dim, spectral_radius=0.95, sparsity=0.1):\n",
    "        # Initialize reservoir weights and input weights with random values\n",
    "        # Apply spectral radius to ensure the echo state property (ESP)\n",
    "        \n",
    "        # Reservoir weights\n",
    "        self.W_res = np.random.rand(reservoir_size, reservoir_size) - 0.5\n",
    "        eigenvalues = np.linalg.eigvals(self.W_res)\n",
    "        self.W_res *= spectral_radius / max(abs(eigenvalues))\n",
    "        \n",
    "        # Input weights\n",
    "        self.W_in = np.random.rand(reservoir_size, input_dim) * 2 - 1\n",
    "        \n",
    "        # Apply sparsity to the reservoir\n",
    "        mask = np.random.rand(reservoir_size, reservoir_size) > sparsity\n",
    "        self.W_res[mask] = 0\n",
    "        \n",
    "        # Set reservoir size, input and output dimensions\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize the reservoir state with zeros\n",
    "        self.state = np.zeros(reservoir_size)\n",
    "        self.W_out = None\n",
    "\n",
    "    def _update(self, input_data):\n",
    "        # Update the reservoir state using the input data and the current state\n",
    "        # This is based on the hyperbolic tangent activation function\n",
    "        self.state = np.tanh(np.dot(self.W_in, input_data) + np.dot(self.W_res, self.state))\n",
    "        return self.state\n",
    "\n",
    "    def fit(self, X, y, reg_coef=1e-6):\n",
    "        # Train the readout weights (W_out) using Ridge Regression\n",
    "        N = len(X)\n",
    "        design_matrix = np.zeros((N, self.reservoir_size))\n",
    "        \n",
    "        # Update reservoir states for each input data\n",
    "        for i in range(N):\n",
    "            design_matrix[i] = self._update(X[i])\n",
    "            \n",
    "        # Compute the readout weights\n",
    "        self.W_out = np.dot(np.linalg.inv(np.dot(design_matrix.T, design_matrix) + \n",
    "                                          reg_coef * np.eye(self.reservoir_size)), \n",
    "                            np.dot(design_matrix.T, y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the output for given input data using the trained readout weights\n",
    "        N = len(X)\n",
    "        output = np.zeros((N, self.output_dim))\n",
    "        \n",
    "        # Compute the output for each input data\n",
    "        for i in range(N):\n",
    "            reservoir_state = self._update(X[i])\n",
    "            output[i] = np.dot(self.W_out.T, reservoir_state)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5f506",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Define the Physics Constraints\n",
    "\n",
    "The Lorenz 96 system is described by a set of ordinary differential equations (ODEs). These equations represent the physics constraints that any model of the system should adhere to. By expressing these ODEs in TensorFlow, we can later integrate them into our neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96c1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_96_derivatives_tf(x, F=8):\n",
    "    N = tf.shape(x)[0]\n",
    "    dxdt = tf.zeros_like(x)\n",
    "    for i in range(N):\n",
    "        dxdt_i = (x[(i + 1) % N] - x[i - 2]) * x[i - 1] - x[i] + F\n",
    "        dxdt = tf.tensor_scatter_nd_update(dxdt, [[i]], [dxdt_i])\n",
    "    return dxdt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd4705",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Design the Neural Network Architecture\n",
    "\n",
    "To predict the derivatives of the Lorenz 96 system, we'll construct a neural network using TensorFlow. This neural network will be used in tandem with the physics constraints defined earlier. The aim is to train the neural network to approximate the dynamics of the Lorenz 96 system while ensuring that it adheres to the physical laws captured by the ODEs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33962ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                384       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4869 (19.02 KB)\n",
      "Trainable params: 4869 (19.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Neural Network Architecture for Lorenz 96 System Prediction\n",
    "# We construct a simple feedforward neural network with two hidden layers, each containing 64 neurons.\n",
    "# The input and output sizes both match the dimension of the Lorenz 96 system, which is set to 'N'.\n",
    "\n",
    "N = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(N,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(N)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2904999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "esn = RC_ESN(input_dim=N, reservoir_size=100, output_dim=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb381c",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Embed Physics Constraints as Loss Terms\n",
    "\n",
    "To ensure that our neural network respects the physics of the Lorenz 96 system, we embed the physics constraints as additional loss terms. \n",
    "\n",
    "- `physics_loss`: Computes the mean squared error between the predicted derivatives from the neural network and those computed from the Lorenz 96 ODEs.\n",
    "- `combined_loss`: Combines the standard mean squared error with the physics-based loss to guide the training of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4afd0d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                384       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4869 (19.02 KB)\n",
      "Trainable params: 4869 (19.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def physics_loss(y_true, y_pred):\n",
    "    predicted_derivatives = lorenz_96_derivatives_tf(y_pred)\n",
    "    return tf.reduce_mean(tf.square(predicted_derivatives - y_true))\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    mse_loss = tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "    return mse_loss + physics_loss(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer='adam', loss=combined_loss)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b168a",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Generate Training Data\n",
    "\n",
    "To train our neural network, we require data that captures the dynamics of the Lorenz 96 system. In this step, we'll simulate the Lorenz 96 system to generate training samples. These samples will be used to train our neural network, ensuring that it learns to approximate the system's behavior while adhering to the underlying physics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d600d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = 8\n",
    "# Initial state for generating training data\n",
    "x0_train = F * np.ones(N)\n",
    "x0_train[0] += 0.01  # Perturb the initial condition slightly\n",
    "\n",
    "# Time points for generating training data\n",
    "t_train = np.arange(0.0, 20.0, 0.01)  # For example, you can adjust the range and step size as needed\n",
    "\n",
    "# Solve the differential equations to generate training data using the Lorenz 96 system's derivatives\n",
    "x_train = odeint(lorenz_96_derivatives_tf, x0_train, t_train)\n",
    "\n",
    "# Compute the derivatives for the training data\n",
    "# This represents the change in the system state, which we aim to predict with our neural network\n",
    "dxdt_train = np.array([lorenz_96_derivatives_tf(xi, 0) for xi in x_train])\n",
    "\n",
    "# The input to our neural network will be the system's state at a given time\n",
    "# The target (or label) will be the change in the system's state at the next time step\n",
    "X_train = x_train[:-1]\n",
    "y_train = x_train[1:] - x_train[:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c8cbe",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Train the Neural Network and the ESN\n",
    "\n",
    "After generating the training data, our next step is to train both the neural network and the Echo State Network (ESN). The neural network will learn to approximate the dynamics of the Lorenz 96 system, while the ESN, a type of recurrent neural network, will be trained to predict the system's behavior based on the training samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbae09f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - 2s 10ms/step - loss: 51.1058 - val_loss: 38.6023\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 45.5413 - val_loss: 37.7957\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 44.3456 - val_loss: 38.6138\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 41.7962 - val_loss: 39.5988\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 39.8995 - val_loss: 38.4222\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 37.1026 - val_loss: 38.6769\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 34.4736 - val_loss: 37.7714\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 33.0206 - val_loss: 37.3953\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.6546 - val_loss: 36.8054\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.5335 - val_loss: 37.3223\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.4810 - val_loss: 36.7481\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.4847 - val_loss: 36.5108\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.4602 - val_loss: 36.0154\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.4206 - val_loss: 36.1239\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3333 - val_loss: 36.1388\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.3300 - val_loss: 36.4583\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3197 - val_loss: 36.2488\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2974 - val_loss: 35.9269\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3256 - val_loss: 35.8967\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2990 - val_loss: 36.0911\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2711 - val_loss: 35.6844\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2747 - val_loss: 35.8800\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2747 - val_loss: 35.7682\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2759 - val_loss: 35.5884\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2561 - val_loss: 35.8174\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2544 - val_loss: 35.8930\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2529 - val_loss: 35.7676\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2495 - val_loss: 35.8875\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3039 - val_loss: 35.7846\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.4634 - val_loss: 35.3219\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3743 - val_loss: 35.3573\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3037 - val_loss: 35.6758\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2658 - val_loss: 35.5550\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2323 - val_loss: 35.4006\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2297 - val_loss: 35.4521\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2501 - val_loss: 35.1394\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2305 - val_loss: 35.5420\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2319 - val_loss: 35.5126\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2500 - val_loss: 35.4144\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2359 - val_loss: 35.3427\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2346 - val_loss: 35.6278\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2321 - val_loss: 35.2519\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2255 - val_loss: 35.2996\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2391 - val_loss: 35.1205\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2266 - val_loss: 35.3087\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2338 - val_loss: 35.3244\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2296 - val_loss: 35.2801\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2416 - val_loss: 35.4955\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2345 - val_loss: 35.2853\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2518 - val_loss: 35.3498\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2567 - val_loss: 35.3435\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2538 - val_loss: 35.4120\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2435 - val_loss: 35.1373\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2183 - val_loss: 35.1435\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2871 - val_loss: 35.2379\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2797 - val_loss: 35.0047\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2460 - val_loss: 34.9282\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2233 - val_loss: 35.1772\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2257 - val_loss: 35.0966\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.3016 - val_loss: 35.0424\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2752 - val_loss: 35.0309\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2184 - val_loss: 34.9878\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2143 - val_loss: 34.8783\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2402 - val_loss: 34.8872\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2405 - val_loss: 34.9514\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2480 - val_loss: 35.1378\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2317 - val_loss: 34.8311\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2191 - val_loss: 34.6588\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2397 - val_loss: 34.6427\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2275 - val_loss: 34.7944\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2056 - val_loss: 34.8801\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2204 - val_loss: 34.8538\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2141 - val_loss: 34.7179\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2236 - val_loss: 34.7223\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2274 - val_loss: 34.9100\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2187 - val_loss: 34.5236\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2365 - val_loss: 34.7212\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2091 - val_loss: 34.7737\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2095 - val_loss: 34.7155\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2124 - val_loss: 34.7436\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2064 - val_loss: 34.8858\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2139 - val_loss: 34.5340\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2135 - val_loss: 34.6720\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2165 - val_loss: 34.6541\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2538 - val_loss: 34.4592\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2906 - val_loss: 34.6563\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2238 - val_loss: 34.5955\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2191 - val_loss: 34.6314\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 32.2178 - val_loss: 34.5412\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2105 - val_loss: 34.5648\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2145 - val_loss: 34.5612\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2333 - val_loss: 34.6320\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2330 - val_loss: 34.3881\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2174 - val_loss: 34.5653\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2082 - val_loss: 34.5815\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2218 - val_loss: 34.4699\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2405 - val_loss: 34.4628\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2427 - val_loss: 34.4870\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2937 - val_loss: 34.4312\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2311 - val_loss: 34.2423\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2180 - val_loss: 34.4186\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2186 - val_loss: 34.2903\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2692 - val_loss: 34.4694\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2239 - val_loss: 34.2289\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2302 - val_loss: 34.2598\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2169 - val_loss: 34.2168\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2238 - val_loss: 34.2691\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2046 - val_loss: 34.3281\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2012 - val_loss: 34.2201\n",
      "Epoch 110/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2038 - val_loss: 34.1648\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2070 - val_loss: 34.3308\n",
      "Epoch 112/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2188 - val_loss: 34.1941\n",
      "Epoch 113/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2170 - val_loss: 34.1125\n",
      "Epoch 114/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2082 - val_loss: 34.2209\n",
      "Epoch 115/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2109 - val_loss: 34.0645\n",
      "Epoch 116/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2250 - val_loss: 34.1435\n",
      "Epoch 117/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2112 - val_loss: 34.0784\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2017 - val_loss: 34.0702\n",
      "Epoch 119/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1977 - val_loss: 34.1963\n",
      "Epoch 120/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2057 - val_loss: 34.1498\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2072 - val_loss: 34.1487\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1984 - val_loss: 34.1646\n",
      "Epoch 123/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2047 - val_loss: 34.1007\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1983 - val_loss: 34.0197\n",
      "Epoch 125/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2079 - val_loss: 34.0827\n",
      "Epoch 126/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2113 - val_loss: 34.1218\n",
      "Epoch 127/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2045 - val_loss: 34.1026\n",
      "Epoch 128/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2510 - val_loss: 34.1067\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2850 - val_loss: 34.0016\n",
      "Epoch 130/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2130 - val_loss: 34.0467\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2004 - val_loss: 34.1016\n",
      "Epoch 132/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1980 - val_loss: 34.0013\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1947 - val_loss: 33.8907\n",
      "Epoch 134/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1960 - val_loss: 34.0213\n",
      "Epoch 135/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2024 - val_loss: 33.9449\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.1961 - val_loss: 33.9387\n",
      "Epoch 137/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2241 - val_loss: 33.8619\n",
      "Epoch 138/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2007 - val_loss: 33.9884\n",
      "Epoch 139/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2096 - val_loss: 33.9343\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2138 - val_loss: 33.9971\n",
      "Epoch 141/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2146 - val_loss: 33.9561\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2294 - val_loss: 33.8666\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2035 - val_loss: 33.8626\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1979 - val_loss: 33.8978\n",
      "Epoch 145/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1969 - val_loss: 33.8535\n",
      "Epoch 146/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2184 - val_loss: 33.7670\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2209 - val_loss: 34.0502\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2051 - val_loss: 33.8762\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2122 - val_loss: 33.7246\n",
      "Epoch 150/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2015 - val_loss: 33.7750\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2077 - val_loss: 33.7671\n",
      "Epoch 152/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2013 - val_loss: 33.7828\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1899 - val_loss: 33.8197\n",
      "Epoch 154/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1901 - val_loss: 33.8013\n",
      "Epoch 155/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1941 - val_loss: 33.8919\n",
      "Epoch 156/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2042 - val_loss: 33.8042\n",
      "Epoch 157/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1980 - val_loss: 33.7528\n",
      "Epoch 158/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.1973 - val_loss: 33.7579\n",
      "Epoch 159/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1941 - val_loss: 33.7455\n",
      "Epoch 160/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2065 - val_loss: 33.6879\n",
      "Epoch 161/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2011 - val_loss: 33.7830\n",
      "Epoch 162/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2012 - val_loss: 33.8441\n",
      "Epoch 163/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1981 - val_loss: 33.7908\n",
      "Epoch 164/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1898 - val_loss: 33.6973\n",
      "Epoch 165/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.1921 - val_loss: 33.7628\n",
      "Epoch 166/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1953 - val_loss: 33.7614\n",
      "Epoch 167/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1998 - val_loss: 33.7475\n",
      "Epoch 168/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2027 - val_loss: 33.6811\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1939 - val_loss: 33.7021\n",
      "Epoch 170/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2060 - val_loss: 33.6711\n",
      "Epoch 171/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1922 - val_loss: 33.7060\n",
      "Epoch 172/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1944 - val_loss: 33.6415\n",
      "Epoch 173/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.2021 - val_loss: 33.6430\n",
      "Epoch 174/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2197 - val_loss: 33.7071\n",
      "Epoch 175/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2071 - val_loss: 33.6090\n",
      "Epoch 176/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1971 - val_loss: 33.6024\n",
      "Epoch 177/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1914 - val_loss: 33.6456\n",
      "Epoch 178/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1891 - val_loss: 33.5913\n",
      "Epoch 179/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1974 - val_loss: 33.5732\n",
      "Epoch 180/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.1941 - val_loss: 33.5839\n",
      "Epoch 181/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2023 - val_loss: 33.6083\n",
      "Epoch 182/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1956 - val_loss: 33.5648\n",
      "Epoch 183/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2026 - val_loss: 33.5178\n",
      "Epoch 184/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2040 - val_loss: 33.5852\n",
      "Epoch 185/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2019 - val_loss: 33.5319\n",
      "Epoch 186/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2000 - val_loss: 33.5476\n",
      "Epoch 187/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.1943 - val_loss: 33.5748\n",
      "Epoch 188/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2004 - val_loss: 33.5845\n",
      "Epoch 189/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2041 - val_loss: 33.5728\n",
      "Epoch 190/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1939 - val_loss: 33.4941\n",
      "Epoch 191/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2015 - val_loss: 33.5207\n",
      "Epoch 192/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1905 - val_loss: 33.5504\n",
      "Epoch 193/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1901 - val_loss: 33.5074\n",
      "Epoch 194/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 32.1969 - val_loss: 33.4387\n",
      "Epoch 195/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1915 - val_loss: 33.5286\n",
      "Epoch 196/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1923 - val_loss: 33.5140\n",
      "Epoch 197/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1983 - val_loss: 33.4947\n",
      "Epoch 198/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.2005 - val_loss: 33.4578\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1897 - val_loss: 33.4901\n",
      "Epoch 200/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 32.1898 - val_loss: 33.5362\n"
     ]
    }
   ],
   "source": [
    "# Train the Neural Network using the generated training data\n",
    "# Note: You might need to adjust the number of epochs, batch size, etc. based on your dataset and problem complexity\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Train the Echo State Network (ESN) using the same training data\n",
    "esn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf1d0c",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: Evaluate and Use the Hybrid Model\n",
    "\n",
    "After training, it's essential to evaluate the model's performance on unseen data to validate its accuracy and generalization capabilities. By evaluating the model on a test dataset, we can gauge how well it has learned to predict the dynamics of the Lorenz 96 system.\n",
    "\n",
    "Once validated, the trained PINN can be used for various tasks related to the Lorenz 96 system, such as forecasting, sensitivity analysis, or even control applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb15538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 34.2198\n",
      "Neural Network Test Loss: 34.21976852416992\n",
      "ESN Mean Squared Error: 0.5241454340204513\n"
     ]
    }
   ],
   "source": [
    "# Initial state for test data\n",
    "x0_test = F * np.ones(N)\n",
    "x0_test[0] += 0.02  # Slightly different initial condition from training\n",
    "\n",
    "# Time points for test data\n",
    "t_test = np.arange(0.0, 30.0, 0.01)\n",
    "\n",
    "# Solve the differential equations for test data using the Lorenz 96 system's derivatives\n",
    "x_test = odeint(lorenz_96_derivatives_tf, x0_test, t_test)\n",
    "\n",
    "# Compute the derivatives for test data\n",
    "dxdt_test = np.array([lorenz_96_derivatives_tf(xi, 0) for xi in x_test])\n",
    "\n",
    "# Prepare test data for evaluation\n",
    "X_test = x_test[:-1]\n",
    "y_test = x_test[1:] - x_test[:-1]\n",
    "\n",
    "# Evaluate the Neural Network on test data\n",
    "nn_test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Neural Network Test Loss: {nn_test_loss}\")\n",
    "\n",
    "# Use ESN to make predictions on test data\n",
    "esn_predictions = esn.predict(X_test)\n",
    "\n",
    "# Compute the Mean Squared Error for ESN predictions\n",
    "esn_mse = ((y_test - esn_predictions) ** 2).mean()\n",
    "print(f\"ESN Mean Squared Error: {esn_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b914d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
